import lancedb
from sentence_transformers import SentenceTransformer
import numpy as np
import pyarrow as pa


# Glossary for vector_search.py

# Vector:
#   An ordered list of numbers representing a point or direction in space.
#   In this program, each vector numerically represents a sentence.

# Embedding:
#   A vector generated by the model that captures the meaning of text
#   in high-dimensional space, allowing semantic comparison.
# 1-dimensional array (vector) of numbers

# SentenceTransformer:
#   The class that loads a pretrained model and converts text into embeddings.

# Model:
#   The trained neural network that maps sentences with similar meanings
#   to nearby vectors in embedding space.

# Normalize:
#   Scaling each embedding so its length (magnitude) equals 1.
#   This focuses comparisons on direction rather than distance.

# Magnitude (or norm):
#   The length of a vector from the origin, calculated as the square root
#   of the sum of squares of its components.

# Cosine similarity:
#   A measure of how close two vectors point in the same direction.
#   1 means identical direction; 0 means unrelated.

# Dot product:
#   The mathematical operation used to compute similarity between vectors.
#   When vectors are normalized, it equals cosine similarity.

# Embedding space:
#   The high-dimensional coordinate system where each sentence is represented as a point.


# Chunking:
#   The process of splitting a long document into smaller pieces (chunks)
#   so that each chunk can be embedded and searched individually.

# Chunk Size:
#   The number of words, sentences, or tokens in each chunk.
#   Determines granularity of search; smaller chunks give more precise matches,
#   but more chunks can increase storage and computation.

# Overlap:
#   Optional repeated content between consecutive chunks.
#   Helps prevent splitting important context across chunks.

# Token:
#   A unit of text used in NLP models.
#   Usually words, subwords, or characters depending on the tokenizer.

# Embedding per Chunk:
#   Each chunk is passed through a model like SentenceTransformer
#   to generate a numerical vector (embedding) representing its meaning.

# Chunked Search:
#   When a query is embedded, it is compared against the embeddings of all chunks
#   instead of whole documents, improving relevance of search results.


#Sentence Transformer invocation with model argument, This model’s internal weights and tokenizer define how text will be turned into embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")


#dummy documents list
documents = [
  "The cat sat on the mat.",
  "A dog barked loudly outside",
  "Artificial intelligence is transforming industries",
  "Frank plays piano"
]


# Encode the list of documents into numerical embeddings (vectors).
# Each document becomes a 384-dimensional vector capturing its meaning.
# The 'normalize_embeddings=True' option scales each vector to length 1,
# so similarity comparisons use direction only (cosine similarity).
# The [0] extracts the first embedding if only one document was passed.
doc_embeddings = model.encode(documents, normalize_embeddings=True)



#Lance DB
def get_or_create_docs_table(path="./search_db"):
    db = lancedb.connect(path)

    # Check if table exists
    if "docs" in db.table_names():
        table = db.open_table("docs")
    else:
        schema = pa.schema([
            pa.field("id", pa.int32()),
            pa.field("text", pa.string()),
            pa.field("embedding", pa.list_(pa.float32(), 384))
        ])
        table = db.create_table("docs", schema=schema)

    return table

table = get_or_create_docs_table()

for i, doc in enumerate(documents):
  table.add([{
    "id": i,
    "text": doc,
    "embedding": doc_embeddings[i].tolist()
  }])




print("doc embeddings:",doc_embeddings)

query = input("Enter your search query: ")

#Embed query
# The model always returns a list or array of embeddings, one for each item in the input list.
# Even if there’s only one item, the output shape is [[v1, v2, v3, ...]] (2D array).
# The [0] extracts the single embedding vector (inner list), giving a 1D array
# that represents the semantic meaning of the query.
query_embedding = model.encode([query], normalize_embeddings=True)[0]



#Compute cosine similarities
scores = np.dot(doc_embeddings, query_embedding)


#get best match
best_index = np.argmax(scores)
print(f"\nTop result: {documents[best_index]}")
print(f"Score: {scores[best_index]:.3f}")
